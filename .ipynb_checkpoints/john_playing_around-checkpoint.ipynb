{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b205e59",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb76bad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md                         playing_around.ipynb\r\n",
      "Sarcasm_Headlines_Dataset_v2.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ce8ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install pandas matplotlib numpy nltk sklearn keras tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8073a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-11 14:41:57.058182: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99359e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"./Sarcasm_Headlines_Dataset_v2.json\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2319011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28614</th>\n",
       "      <td>1</td>\n",
       "      <td>jews to celebrate rosh hashasha or something</td>\n",
       "      <td>https://www.theonion.com/jews-to-celebrate-ros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28615</th>\n",
       "      <td>1</td>\n",
       "      <td>internal affairs investigator disappointed con...</td>\n",
       "      <td>https://local.theonion.com/internal-affairs-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28616</th>\n",
       "      <td>0</td>\n",
       "      <td>the most beautiful acceptance speech this week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/andrew-ah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28617</th>\n",
       "      <td>1</td>\n",
       "      <td>mars probe destroyed by orbiting spielberg-gat...</td>\n",
       "      <td>https://www.theonion.com/mars-probe-destroyed-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28618</th>\n",
       "      <td>1</td>\n",
       "      <td>dad clarifies this not a food stop</td>\n",
       "      <td>https://www.theonion.com/dad-clarifies-this-no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28619 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       is_sarcastic                                           headline  \\\n",
       "0                 1  thirtysomething scientists unveil doomsday clo...   \n",
       "1                 0  dem rep. totally nails why congress is falling...   \n",
       "2                 0  eat your veggies: 9 deliciously different recipes   \n",
       "3                 1  inclement weather prevents liar from getting t...   \n",
       "4                 1  mother comes pretty close to using word 'strea...   \n",
       "...             ...                                                ...   \n",
       "28614             1       jews to celebrate rosh hashasha or something   \n",
       "28615             1  internal affairs investigator disappointed con...   \n",
       "28616             0  the most beautiful acceptance speech this week...   \n",
       "28617             1  mars probe destroyed by orbiting spielberg-gat...   \n",
       "28618             1                 dad clarifies this not a food stop   \n",
       "\n",
       "                                            article_link  \n",
       "0      https://www.theonion.com/thirtysomething-scien...  \n",
       "1      https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2      https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3      https://local.theonion.com/inclement-weather-p...  \n",
       "4      https://www.theonion.com/mother-comes-pretty-c...  \n",
       "...                                                  ...  \n",
       "28614  https://www.theonion.com/jews-to-celebrate-ros...  \n",
       "28615  https://local.theonion.com/internal-affairs-in...  \n",
       "28616  https://www.huffingtonpost.com/entry/andrew-ah...  \n",
       "28617  https://www.theonion.com/mars-probe-destroyed-...  \n",
       "28618  https://www.theonion.com/dad-clarifies-this-no...  \n",
       "\n",
       "[28619 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9f4f12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28619</td>\n",
       "      <td>28619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>28503</td>\n",
       "      <td>28617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>'no way to prevent this,' says only nation whe...</td>\n",
       "      <td>https://politics.theonion.com/nation-not-sure-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 headline  \\\n",
       "count                                               28619   \n",
       "unique                                              28503   \n",
       "top     'no way to prevent this,' says only nation whe...   \n",
       "freq                                                   12   \n",
       "\n",
       "                                             article_link  \n",
       "count                                               28619  \n",
       "unique                                              28617  \n",
       "top     https://politics.theonion.com/nation-not-sure-...  \n",
       "freq                                                    2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for duplicate headlines\n",
    "data.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6459d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate headlines\n",
    "data=data.drop(data[data['headline'].duplicated()].index,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39654b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13552 sarcastic headlines and 14951 non-sarcastic headlines\n"
     ]
    }
   ],
   "source": [
    "sarc_cnt = len(data.query('is_sarcastic==1'))\n",
    "non_sarc_cnt = len(data.query('is_sarcastic==0'))\n",
    "\n",
    "# Summary of sarcastic lines\n",
    "print(f'There are {sarc_cnt} sarcastic headlines and {non_sarc_cnt} non-sarcastic headlines')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe773c72",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e473f3",
   "metadata": {},
   "source": [
    "## Part 2: Data Processing/Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b61911e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stopwords from nltk\n",
    "stwrds = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8b94e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to clean a given headline by lowercasing the string, removing spaces, and removing stopwords\n",
    "def clean_headlines(headline):\n",
    "    headline = headline.lower()\n",
    "    headline_split = headline.split()\n",
    "    cleaned_headline = []\n",
    "    for word in headline_split:\n",
    "        if word not in stwrds:\n",
    "            cleaned_headline.append(word)\n",
    "    \n",
    "    cleaned_line = \" \".join(cleaned_headline)\n",
    "    return cleaned_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf55ecb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        thirtysomething scientists unveil doomsday clo...\n",
       "1        dem rep. totally nails congress falling short ...\n",
       "2             eat veggies: 9 deliciously different recipes\n",
       "3             inclement weather prevents liar getting work\n",
       "4        mother comes pretty close using word 'streamin...\n",
       "                               ...                        \n",
       "28614               jews celebrate rosh hashasha something\n",
       "28615    internal affairs investigator disappointed con...\n",
       "28616    beautiful acceptance speech week came queer ko...\n",
       "28617    mars probe destroyed orbiting spielberg-gates ...\n",
       "28618                              dad clarifies food stop\n",
       "Name: headline, Length: 28503, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies function to all entries\n",
    "data['headline'].apply(clean_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01117f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "headline_target = data['is_sarcastic']\n",
    "headline_attributes = data['headline']\n",
    "attribute_train, attribute_test, labels_train, labels_test = train_test_split(headline_attributes, headline_target, test_size=0.30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22762ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form dataframes for training and test sets\n",
    "att_train = pd.DataFrame(attribute_train)\n",
    "label_train = pd.DataFrame(labels_train)\n",
    "att_test = pd.DataFrame(attribute_test)\n",
    "label_test = pd.DataFrame(labels_test)\n",
    "\n",
    "training_set = label_train.join(att_train)\n",
    "test_set = label_test.join(att_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "597bf06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38234 unqiue words in the headline data\n"
     ]
    }
   ],
   "source": [
    "# determines amount of unqiue words in our data\n",
    "# this takes forever because set operations are slow\n",
    "\n",
    "unqiue_words = set()\n",
    "for headline in headline_attributes:\n",
    "    unqiue_words = unqiue_words.union(set(headline.split()))\n",
    "\n",
    "print(f'{len(unqiue_words)} unqiue words in the headline data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1932bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given that there are over 38000 unqiue words a 20,000 vocab size seems appropriate\n",
    "vocab_size = 20000\n",
    "\n",
    "# initalize tokenizer and fit encodings to our dataset\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(data['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c619c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and sequence the training and test splits\n",
    "train_seqs = tokenizer.texts_to_sequences(training_set['headline'])\n",
    "train_pad = pad_sequences(train_seqs, maxlen=50, padding='post', truncating='post')\n",
    "\n",
    "test_seqs = tokenizer.texts_to_sequences(test_set['headline'])\n",
    "test_pad = pad_sequences(test_seqs, maxlen=50, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a3c692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
